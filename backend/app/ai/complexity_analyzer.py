"""
å¤æ‚åº¦åˆ†æå™¨
åŠ¨æ€è®¡ç®—å¯¹è¯è¯·æ±‚å¤æ‚åº¦ï¼Œä¼˜åŒ–AIè·¯ç”±å†³ç­–
"""

import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from dataclasses import dataclass
from collections import defaultdict

from loguru import logger

from app.ai.orchestrator import IntentType, IntentClassification


@dataclass
class ComplexityFactors:
    """å¤æ‚åº¦å½±å“å› ç´ """
    base_score: int  # åŸºç¡€åˆ†æ•°(åŸºäºæ„å›¾)
    context_adjustment: int  # ä¸Šä¸‹æ–‡è°ƒæ•´
    user_pattern_adjustment: int  # ç”¨æˆ·æ¨¡å¼è°ƒæ•´
    conversation_depth_adjustment: int  # å¯¹è¯æ·±åº¦è°ƒæ•´
    technical_level_adjustment: int  # ä¸“ä¸šç¨‹åº¦è°ƒæ•´

    @property
    def total_score(self) -> int:
        """è®¡ç®—æ€»åˆ†"""
        return max(1, min(10,
            self.base_score +
            self.context_adjustment +
            self.user_pattern_adjustment +
            self.conversation_depth_adjustment +
            self.technical_level_adjustment
        ))


@dataclass
class UserBehaviorProfile:
    """ç”¨æˆ·è¡Œä¸ºç”»åƒ"""
    days_active: int
    total_conversations: int
    avg_complexity_history: float
    is_power_user: bool
    expertise_level: str  # "beginner" | "intermediate" | "advanced"
    preferred_interaction_style: str  # "brief" | "detailed"


class ComplexityAnalyzer:
    """
    æ™ºèƒ½å¤æ‚åº¦åˆ†æå™¨

    åŠŸèƒ½:
    1. å¯¹è¯ä¸Šä¸‹æ–‡åˆ†æ (å¤šè½®ã€ä¸»é¢˜åˆ‡æ¢ã€å†å²å¼•ç”¨)
    2. ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ (ç»éªŒç­‰çº§ã€ä¸“ä¸šç¨‹åº¦)
    3. å®æ—¶æ€§èƒ½ç›‘æ§ (å“åº”æ—¶é•¿ã€æˆæœ¬æ•ˆç›Š)
    4. åŠ¨æ€é˜ˆå€¼è°ƒæ•´ (åŸºäºå†å²æ•°æ®å­¦ä¹ )
    """

    # æ„å›¾åŸºç¡€åˆ†æ•°æ˜ å°„
    INTENT_BASE_SCORES = {
        IntentType.GREETING: 1,
        IntentType.CONFIRMATION: 1,
        IntentType.DATA_QUERY: 3,
        IntentType.ADVICE_REQUEST: 5,
        IntentType.EMOTIONAL_SUPPORT: 6,
        IntentType.COMPLEX_ANALYSIS: 8,
        IntentType.HEALTH_DIAGNOSIS: 9,
    }

    # ä¸“ä¸šæœ¯è¯­å…³é”®è¯ (è¡¨ç¤ºç”¨æˆ·ä¸“ä¸šç¨‹åº¦é«˜)
    TECHNICAL_KEYWORDS = {
        "å¿ƒç‡å˜å¼‚æ€§", "hrv", "rhr", "resting heart rate",
        "æ·±åº¦ç¡çœ ", "rem", "å¿«é€Ÿçœ¼åŠ¨",
        "åŸºç¡€ä»£è°¢", "bmr", "tdee",
        "çš®è´¨é†‡", "è¤ªé»‘ç´ ", "è¡€æ¸…ç´ ",
        "æœ‰æ°§è¿åŠ¨", "æ— æ°§è¿åŠ¨", "hiit",
        "æ˜¼å¤œèŠ‚å¾‹", "ç”Ÿç‰©é’Ÿ",
        "ç„¦è™‘éšœç¢", "æŠ‘éƒç—‡", "è®¤çŸ¥è¡Œä¸ºç–—æ³•"
    }

    def __init__(self):
        """åˆå§‹åŒ–å¤æ‚åº¦åˆ†æå™¨"""
        # å†å²å†³ç­–è®°å½• (ç”¨äºåŠ¨æ€å­¦ä¹ )
        self.decision_history: List[Dict] = []

        # ç”¨æˆ·è¡Œä¸ºç¼“å­˜
        self.user_profiles: Dict[str, UserBehaviorProfile] = {}

        # æ€§èƒ½ç»Ÿè®¡
        self.analysis_count = 0
        self.avg_analysis_time_ms = 0.0

        logger.info("âœ… ComplexityAnalyzer initialized")

    async def analyze_complexity(
        self,
        intent: IntentClassification,
        user_message: str,
        conversation_history: Optional[List[Dict]] = None,
        user_profile: Optional[Dict] = None,
        user_id: Optional[str] = None
    ) -> ComplexityFactors:
        """
        åˆ†æè¯·æ±‚å¤æ‚åº¦

        Args:
            intent: æ„å›¾åˆ†ç±»ç»“æœ
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²
            user_profile: ç”¨æˆ·ç”»åƒ
            user_id: ç”¨æˆ·ID

        Returns:
            ComplexityFactors: å¤æ‚åº¦å› ç´ è¯¦æƒ…
        """
        start_time = datetime.now()

        # 1. åŸºç¡€åˆ†æ•° (åŸºäºæ„å›¾)
        base_score = self.INTENT_BASE_SCORES.get(intent.intent, 5)

        # 2. ä¸Šä¸‹æ–‡å¤æ‚åº¦åˆ†æ
        context_adjustment = self._analyze_context_complexity(
            user_message,
            conversation_history
        )

        # 3. ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ
        user_pattern_adjustment = await self._analyze_user_pattern(
            user_id,
            user_profile
        )

        # 4. å¯¹è¯æ·±åº¦åˆ†æ
        conversation_depth_adjustment = self._analyze_conversation_depth(
            conversation_history
        )

        # 5. ä¸“ä¸šç¨‹åº¦åˆ†æ
        technical_level_adjustment = self._analyze_technical_level(
            user_message
        )

        # æ„å»ºå¤æ‚åº¦å› ç´ 
        factors = ComplexityFactors(
            base_score=base_score,
            context_adjustment=context_adjustment,
            user_pattern_adjustment=user_pattern_adjustment,
            conversation_depth_adjustment=conversation_depth_adjustment,
            technical_level_adjustment=technical_level_adjustment
        )

        # è®°å½•åˆ†ææ—¶é•¿
        analysis_time = (datetime.now() - start_time).total_seconds() * 1000
        self.analysis_count += 1
        self.avg_analysis_time_ms = (
            (self.avg_analysis_time_ms * (self.analysis_count - 1) + analysis_time)
            / self.analysis_count
        )

        logger.debug(
            f"ğŸ” Complexity: {factors.total_score} | "
            f"Base: {base_score} | "
            f"Context: +{context_adjustment} | "
            f"User: +{user_pattern_adjustment} | "
            f"Depth: +{conversation_depth_adjustment} | "
            f"Tech: +{technical_level_adjustment}"
        )

        return factors

    def _analyze_context_complexity(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict]]
    ) -> int:
        """
        åˆ†æä¸Šä¸‹æ–‡å¤æ‚åº¦

        è€ƒè™‘å› ç´ :
        - æ¶ˆæ¯é•¿åº¦
        - é—®é¢˜æ•°é‡
        - æ¡ä»¶å¥æ•°é‡
        - ä¸Šä¸‹æ–‡é•¿åº¦
        """
        adjustment = 0

        # æ¶ˆæ¯é•¿åº¦
        message_length = len(user_message)
        if message_length > 200:
            adjustment += 2
        elif message_length > 100:
            adjustment += 1

        # é—®é¢˜æ•°é‡ (å¤šä¸ªé—®é¢˜ â†’ æ›´å¤æ‚)
        question_count = user_message.count('?') + user_message.count('ï¼Ÿ')
        if question_count >= 3:
            adjustment += 2
        elif question_count >= 2:
            adjustment += 1

        # æ¡ä»¶å¥ (å¦‚æœ...é‚£ä¹ˆ... â†’ é€»è¾‘å¤æ‚)
        conditional_keywords = ['å¦‚æœ', 'å‡å¦‚', 'è¦æ˜¯', 'if', 'ä½†æ˜¯', 'ä¸è¿‡', 'ç„¶è€Œ']
        conditional_count = sum(1 for kw in conditional_keywords if kw in user_message.lower())
        if conditional_count >= 2:
            adjustment += 1

        # ä¸Šä¸‹æ–‡é•¿åº¦
        if conversation_history:
            history_length = len(conversation_history)
            total_chars = sum(len(msg.get("content", "")) for msg in conversation_history)

            if history_length > 10:
                adjustment += 2
            elif history_length > 5:
                adjustment += 1

            if total_chars > 2000:
                adjustment += 1

        return adjustment

    async def _analyze_user_pattern(
        self,
        user_id: Optional[str],
        user_profile: Optional[Dict]
    ) -> int:
        """
        åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼

        è€ƒè™‘å› ç´ :
        - ç”¨æˆ·æ´»è·ƒå¤©æ•°
        - å†å²æŸ¥è¯¢å¤æ‚åº¦
        - æ˜¯å¦ä¸ºé«˜çº§ç”¨æˆ·
        """
        adjustment = 0

        if not user_id or not user_profile:
            return 0

        # è·å–æˆ–åˆ›å»ºç”¨æˆ·ç”»åƒ
        if user_id not in self.user_profiles:
            behavior_profile = self._build_user_behavior_profile(user_profile)
            self.user_profiles[user_id] = behavior_profile
        else:
            behavior_profile = self.user_profiles[user_id]

        # æ–°ç”¨æˆ· â†’ å¯èƒ½éœ€è¦æ›´è¯¦ç»†çš„å›ç­”
        if behavior_profile.days_active < 7:
            adjustment += 1

        # é«˜çº§ç”¨æˆ· â†’ å¯èƒ½æå‡ºæ›´å¤æ‚é—®é¢˜
        if behavior_profile.expertise_level == "advanced":
            adjustment += 1
        elif behavior_profile.expertise_level == "beginner":
            adjustment -= 1

        # é‡åº¦ç”¨æˆ· â†’ å¯èƒ½æœ‰ç‰¹æ®Šéœ€æ±‚
        if behavior_profile.is_power_user:
            adjustment += 1

        return adjustment

    def _build_user_behavior_profile(
        self,
        user_profile: Dict
    ) -> UserBehaviorProfile:
        """
        æ„å»ºç”¨æˆ·è¡Œä¸ºç”»åƒ

        åŸºäºç”¨æˆ·åŸºæœ¬ä¿¡æ¯æ¨æ–­è¡Œä¸ºç‰¹å¾
        """
        days_active = user_profile.get("days_active", 0)

        # ç®€å•å¯å‘å¼åˆ¤æ–­ä¸“ä¸šç¨‹åº¦
        occupation = user_profile.get("occupation", "").lower()
        expertise_level = "beginner"

        if any(kw in occupation for kw in ["åŒ»ç”Ÿ", "æŠ¤å£«", "å¥åº·", "åŒ»ç–—", "doctor", "nurse"]):
            expertise_level = "advanced"
        elif any(kw in occupation for kw in ["æ•™ç»ƒ", "è¿åŠ¨", "fitness", "trainer"]):
            expertise_level = "intermediate"

        # åˆ¤æ–­æ˜¯å¦ä¸ºé‡åº¦ç”¨æˆ·
        is_power_user = days_active > 30

        return UserBehaviorProfile(
            days_active=days_active,
            total_conversations=0,  # éœ€è¦ä»æ•°æ®åº“æŸ¥è¯¢
            avg_complexity_history=5.0,
            is_power_user=is_power_user,
            expertise_level=expertise_level,
            preferred_interaction_style="detailed" if days_active < 7 else "brief"
        )

    def _analyze_conversation_depth(
        self,
        conversation_history: Optional[List[Dict]]
    ) -> int:
        """
        åˆ†æå¯¹è¯æ·±åº¦

        è€ƒè™‘å› ç´ :
        - å¤šè½®å¯¹è¯
        - ä¸»é¢˜è¿ç»­æ€§
        - æ·±å…¥æ¢è®¨ç¨‹åº¦
        """
        adjustment = 0

        if not conversation_history or len(conversation_history) == 0:
            return 0

        history_length = len(conversation_history)

        # å¤šè½®å¯¹è¯ (>3è½®å¯¹è¯è¡¨ç¤ºæ·±å…¥è®¨è®º)
        if history_length > 6:
            adjustment += 2
        elif history_length > 3:
            adjustment += 1

        # æ£€æµ‹ä¸»é¢˜åˆ‡æ¢
        # å¦‚æœæœ€è¿‘3æ¡æ¶ˆæ¯çš„ä¸»é¢˜å·®å¼‚è¾ƒå¤§ â†’ å¤æ‚åº¦æé«˜
        if history_length >= 3:
            recent_messages = conversation_history[-3:]
            topic_switch_detected = self._detect_topic_switch(recent_messages)

            if topic_switch_detected:
                adjustment += 1

        # æ£€æµ‹å†å²å¼•ç”¨ ("ä¹‹å‰ä½ è¯´", "åˆšæ‰æåˆ°")
        last_message = conversation_history[-1].get("content", "") if conversation_history else ""
        reference_keywords = ["ä¹‹å‰", "åˆšæ‰", "å‰é¢", "earlier", "previously", "ä¸Šæ¬¡"]

        if any(kw in last_message for kw in reference_keywords):
            adjustment += 1

        return adjustment

    def _detect_topic_switch(self, recent_messages: List[Dict]) -> bool:
        """
        æ£€æµ‹ä¸»é¢˜åˆ‡æ¢

        ç®€å•å®ç°: æ£€æŸ¥å…³é”®è¯é‡å åº¦
        """
        if len(recent_messages) < 2:
            return False

        # æå–å…³é”®è¯
        def extract_keywords(text: str) -> set:
            # ç®€å•åˆ†è¯ (ç§»é™¤å¸¸è§åœç”¨è¯)
            stopwords = {"çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "åœ¨", "æœ‰", "å’Œ", "å—", "å‘¢", "a", "the", "is", "are"}
            words = set(re.findall(r'[\w]+', text.lower()))
            return words - stopwords

        keywords_1 = extract_keywords(recent_messages[-1].get("content", ""))
        keywords_2 = extract_keywords(recent_messages[-2].get("content", ""))

        # è®¡ç®—é‡å åº¦
        if len(keywords_1) == 0 or len(keywords_2) == 0:
            return False

        overlap = len(keywords_1 & keywords_2)
        overlap_ratio = overlap / max(len(keywords_1), len(keywords_2))

        # é‡å åº¦<30% è®¤ä¸ºä¸»é¢˜åˆ‡æ¢
        return overlap_ratio < 0.3

    def _analyze_technical_level(self, user_message: str) -> int:
        """
        åˆ†æä¸“ä¸šç¨‹åº¦

        è€ƒè™‘å› ç´ :
        - ä¸“ä¸šæœ¯è¯­ä½¿ç”¨
        - æ•°æ®å¼•ç”¨
        - å…·ä½“æŒ‡æ ‡æåŠ
        """
        adjustment = 0

        message_lower = user_message.lower()

        # æ£€æµ‹ä¸“ä¸šæœ¯è¯­
        technical_count = sum(
            1 for keyword in self.TECHNICAL_KEYWORDS
            if keyword in message_lower
        )

        if technical_count >= 3:
            adjustment += 2
        elif technical_count >= 1:
            adjustment += 1

        # æ£€æµ‹æ•°å€¼/æ•°æ®å¼•ç”¨
        # å¦‚: "æˆ‘çš„å¿ƒç‡æ˜¯75", "ç¡çœ æ—¶é•¿6.5å°æ—¶"
        number_pattern = r'\d+\.?\d*\s*(å°æ—¶|åˆ†é’Ÿ|æ¬¡|bpm|kg|cm|%|hours?|mins?)'
        number_matches = re.findall(number_pattern, user_message)

        if len(number_matches) >= 2:
            adjustment += 1

        return adjustment

    def record_decision(
        self,
        complexity: int,
        provider_used: str,
        actual_latency_ms: float,
        actual_cost: float,
        user_satisfaction: Optional[int] = None
    ):
        """
        è®°å½•è·¯ç”±å†³ç­– (ç”¨äºå­¦ä¹ ä¼˜åŒ–)

        Args:
            complexity: è®¡ç®—å‡ºçš„å¤æ‚åº¦
            provider_used: å®é™…ä½¿ç”¨çš„æä¾›å•†
            actual_latency_ms: å®é™…å“åº”æ—¶é•¿
            actual_cost: å®é™…æˆæœ¬
            user_satisfaction: ç”¨æˆ·æ»¡æ„åº¦(1-5)
        """
        self.decision_history.append({
            "timestamp": datetime.now(),
            "complexity": complexity,
            "provider": provider_used,
            "latency_ms": actual_latency_ms,
            "cost": actual_cost,
            "satisfaction": user_satisfaction
        })

        # ä¿ç•™æœ€è¿‘1000æ¡è®°å½•
        if len(self.decision_history) > 1000:
            self.decision_history = self.decision_history[-1000:]

    def get_stats(self) -> Dict:
        """è·å–åˆ†æå™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "analysis_count": self.analysis_count,
            "avg_analysis_time_ms": round(self.avg_analysis_time_ms, 2),
            "user_profiles_cached": len(self.user_profiles),
            "decision_history_size": len(self.decision_history)
        }


# å…¨å±€å•ä¾‹
_global_analyzer: Optional[ComplexityAnalyzer] = None


def get_complexity_analyzer() -> ComplexityAnalyzer:
    """è·å–å…¨å±€ComplexityAnalyzerå•ä¾‹"""
    global _global_analyzer

    if _global_analyzer is None:
        _global_analyzer = ComplexityAnalyzer()

    return _global_analyzer
