"""
æœ¬åœ°AIæ¨¡å‹ç®¡ç†å™¨
ä½¿ç”¨Phi-3.5-mini-instructè¿›è¡Œæœ¬åœ°æ¨ç†ï¼Œå®ç°70%æˆæœ¬èŠ‚çœ
"""

import asyncio
import logging
from typing import Optional, Dict, Any
from datetime import datetime
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from threading import Thread

from app.core.config import settings

logger = logging.getLogger(__name__)


class LocalModelManager:
    """
    æœ¬åœ°Phi-3.5æ¨¡å‹ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    - æ‡’åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨æ—¶åŠ è½½ï¼‰
    - å†…å­˜ä¼˜åŒ–ï¼ˆfloat16, æ¨¡å‹é‡åŒ–ï¼‰
    - å¼‚æ­¥æ¨ç†
    - ç¼“å­˜ç®¡ç†
    """

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = self._detect_device()
        self.model_name = "microsoft/Phi-3.5-mini-instruct"
        self.is_loaded = False
        self.load_lock = asyncio.Lock()

        # æ€§èƒ½æŒ‡æ ‡
        self.inference_count = 0
        self.total_inference_time = 0.0

        logger.info(f"ğŸ§  LocalModelManager initialized (device: {self.device})")

    def _detect_device(self) -> str:
        """
        æ£€æµ‹å¯ç”¨è®¾å¤‡
        ä¼˜å…ˆçº§: CUDA > MPS (Apple Silicon) > CPU
        """
        if torch.cuda.is_available():
            device = "cuda"
            logger.info("âœ… CUDA GPU detected")
        elif torch.backends.mps.is_available():
            device = "mps"
            logger.info("âœ… Apple Silicon MPS detected")
        else:
            device = "cpu"
            logger.warning("âš ï¸  No GPU detected, using CPU (slow)")

        return device

    async def load_model(self) -> None:
        """
        åŠ è½½Phi-3.5æ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼‰
        ä½¿ç”¨float16èŠ‚çœå†…å­˜ï¼Œä½¿ç”¨device_mapè‡ªåŠ¨åˆ†é…è®¾å¤‡
        """
        async with self.load_lock:
            if self.is_loaded:
                return

            logger.info(f"ğŸ“¦ Loading Phi-3.5 model: {self.model_name}")
            start_time = datetime.now()

            try:
                # åœ¨çº¿ç¨‹æ± ä¸­åŠ è½½æ¨¡å‹ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(None, self._load_model_sync)

                load_time = (datetime.now() - start_time).total_seconds()
                logger.info(f"âœ… Model loaded successfully in {load_time:.2f}s")

                # å†…å­˜ä½¿ç”¨æƒ…å†µ
                if self.device == "mps":
                    logger.info("ğŸ’¾ Using Apple Silicon unified memory")
                elif self.device == "cuda":
                    allocated = torch.cuda.memory_allocated() / 1024**3
                    logger.info(f"ğŸ’¾ GPU memory: {allocated:.2f}GB")

                self.is_loaded = True

            except Exception as e:
                logger.error(f"âŒ Failed to load model: {e}", exc_info=True)
                raise RuntimeError(f"Model loading failed: {e}")

    def _load_model_sync(self) -> None:
        """åŒæ­¥åŠ è½½æ¨¡å‹ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )

        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,  # èŠ‚çœå†…å­˜
            device_map=self.device,
            trust_remote_code=True,
            low_cpu_mem_usage=True  # ä¼˜åŒ–CPUå†…å­˜ä½¿ç”¨
        )

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()

    async def generate(
        self,
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        **kwargs
    ) -> str:
        """
        å¼‚æ­¥ç”Ÿæˆå“åº”

        Args:
            prompt: è¾“å…¥æç¤ºè¯
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰
            top_p: nucleus samplingå‚æ•°

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å“åº”
        """
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if not self.is_loaded:
            await self.load_model()

        start_time = datetime.now()

        try:
            # æ„å»ºPhi-3.5çš„å¯¹è¯æ ¼å¼
            formatted_prompt = self._format_phi3_prompt(prompt)

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ¨ç†
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                self._generate_sync,
                formatted_prompt,
                max_new_tokens,
                temperature,
                top_p,
                kwargs
            )

            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            inference_time = (datetime.now() - start_time).total_seconds()
            self.inference_count += 1
            self.total_inference_time += inference_time

            avg_time = self.total_inference_time / self.inference_count
            logger.info(
                f"âš¡ Inference #{self.inference_count}: {inference_time*1000:.0f}ms "
                f"(avg: {avg_time*1000:.0f}ms)"
            )

            return response.strip()

        except Exception as e:
            logger.error(f"âŒ Inference failed: {e}", exc_info=True)
            raise RuntimeError(f"Inference failed: {e}")

    def _format_phi3_prompt(self, user_message: str) -> str:
        """
        æ ¼å¼åŒ–Phi-3.5çš„æç¤ºè¯
        ä½¿ç”¨å®˜æ–¹æ¨èçš„æ ¼å¼: <|user|>\n{content}<|end|>\n<|assistant|>\n
        """
        return f"<|user|>\n{user_message}<|end|>\n<|assistant|>\n"

    def _generate_sync(
        self,
        prompt: str,
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        kwargs: Dict[str, Any]
    ) -> str:
        """åŒæ­¥ç”Ÿæˆï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048  # Phi-3.5ä¸Šä¸‹æ–‡é•¿åº¦
        ).to(self.device)

        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=False,  # ç¦ç”¨cacheä»¥é¿å…DynamicCacheå…¼å®¹æ€§é—®é¢˜
                **kwargs
            )

        # Decode
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],  # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            skip_special_tokens=True
        )

        return generated_text

    async def generate_stream(
        self,
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ):
        """
        æµå¼ç”Ÿæˆï¼ˆæš‚æœªå®ç°ï¼‰
        ç”¨äºæœªæ¥æ”¯æŒæµå¼å“åº”
        """
        # TODO: å®ç°æµå¼ç”Ÿæˆ
        raise NotImplementedError("Streaming not implemented yet")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if self.inference_count == 0:
            return {
                "status": "not_used",
                "inference_count": 0
            }

        avg_time = self.total_inference_time / self.inference_count
        return {
            "status": "active" if self.is_loaded else "not_loaded",
            "device": self.device,
            "model_name": self.model_name,
            "inference_count": self.inference_count,
            "total_time": round(self.total_inference_time, 2),
            "avg_inference_time_ms": round(avg_time * 1000, 0),
            "memory_allocated_gb": (
                round(torch.cuda.memory_allocated() / 1024**3, 2)
                if self.device == "cuda" else None
            )
        }

    async def unload_model(self) -> None:
        """å¸è½½æ¨¡å‹ï¼ˆé‡Šæ”¾å†…å­˜ï¼‰"""
        if not self.is_loaded:
            return

        logger.info("ğŸ—‘ï¸  Unloading Phi-3.5 model")

        self.model = None
        self.tokenizer = None
        self.is_loaded = False

        # æ¸…ç†GPUç¼“å­˜
        if self.device == "cuda":
            torch.cuda.empty_cache()

        logger.info("âœ… Model unloaded")


# å…¨å±€å•ä¾‹
_local_model_manager: Optional[LocalModelManager] = None


def get_local_model_manager() -> LocalModelManager:
    """è·å–å…¨å±€LocalModelManagerå•ä¾‹"""
    global _local_model_manager

    if _local_model_manager is None:
        _local_model_manager = LocalModelManager()

    return _local_model_manager
