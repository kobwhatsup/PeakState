"""
AI Orchestrator - æ™ºèƒ½è·¯ç”±æ ¸å¿ƒ
æ ¹æ®è¯·æ±‚å¤æ‚åº¦å’Œæ„å›¾,è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„AIæ¨¡å‹
å®ç°æˆæœ¬ä¼˜åŒ–å’Œæ€§èƒ½ä¼˜åŒ–çš„å¹³è¡¡
"""

from enum import Enum
from typing import Optional, Tuple, List, Dict, Any
from dataclasses import dataclass
import asyncio
from loguru import logger

from app.core.config import settings


class AIProvider(str, Enum):
    """AIæä¾›å•†æšä¸¾"""
    LOCAL_PHI = "phi-3.5"
    OPENAI_GPT5 = "gpt-5"  # GPT-5 æ——èˆ°æ¨¡å‹
    OPENAI_GPT5_NANO = "gpt-5-nano-2025-08-07"  # GPT-5 Nano æœ€æ–°ç‰ˆæœ¬
    CLAUDE_SONNET_4 = "claude-sonnet-4-20250514"  # Claude Sonnet 4 æœ€æ–°ç‰ˆæœ¬


class IntentType(str, Enum):
    """æ„å›¾ç±»å‹"""
    GREETING = "greeting"  # é—®å€™
    CONFIRMATION = "confirmation"  # ç¡®è®¤
    DATA_QUERY = "data_query"  # æ•°æ®æŸ¥è¯¢
    ADVICE_REQUEST = "advice_request"  # å»ºè®®è¯·æ±‚
    EMOTIONAL_SUPPORT = "emotional_support"  # æƒ…æ„Ÿæ”¯æŒ
    COMPLEX_ANALYSIS = "complex_analysis"  # å¤æ‚åˆ†æ
    HEALTH_DIAGNOSIS = "health_diagnosis"  # å¥åº·è¯Šæ–­


@dataclass
class IntentClassification:
    """æ„å›¾åˆ†ç±»ç»“æœ"""
    intent: IntentType
    confidence: float
    requires_empathy: bool = False  # æ˜¯å¦éœ€è¦æƒ…æ„Ÿç†è§£
    requires_tools: bool = False  # æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
    requires_rag: bool = False  # æ˜¯å¦éœ€è¦RAGå¢å¼º


@dataclass
class RoutingDecision:
    """è·¯ç”±å†³ç­–ç»“æœ"""
    provider: AIProvider
    complexity: int  # 1-10
    estimated_cost: float  # é¢„ä¼°æˆæœ¬(ç¾å…ƒ)
    estimated_latency: float  # é¢„ä¼°å»¶è¿Ÿ(ç§’)
    reason: str  # è·¯ç”±åŸå› 
    intent: IntentClassification  # æ„å›¾åˆ†ç±»ç»“æœ


@dataclass
class AIResponse:
    """AIå“åº”ç»“æœ"""
    content: str  # å“åº”å†…å®¹
    tokens_used: Optional[int] = None  # ä½¿ç”¨çš„tokenæ•°
    finish_reason: Optional[str] = None  # å®ŒæˆåŸå› 


class AIOrchestrator:
    """
    AIç¼–æ’å™¨
    è´Ÿè´£æ™ºèƒ½è·¯ç”±ã€è¯·æ±‚åˆ†å‘ã€å“åº”èšåˆ
    """

    def __init__(self):
        """åˆå§‹åŒ–ç¼–æ’å™¨"""
        self.local_model = None  # å»¶è¿ŸåŠ è½½
        self.openai_client = None
        self.anthropic_client = None

        # æˆæœ¬é…ç½®(æ¯1K tokens) - åŸºäº2025å¹´æœ€æ–°å®šä»·
        self.cost_config = {
            AIProvider.LOCAL_PHI: 0.0,
            AIProvider.OPENAI_GPT5_NANO: 0.0002,  # $0.2/1M tokens (è¶…ç»æµ)
            AIProvider.OPENAI_GPT5: 0.005,  # $5/1M tokens (æ——èˆ°çº§)
            AIProvider.CLAUDE_SONNET_4: 0.003,  # $3/1M tokens
        }

        # å»¶è¿Ÿé…ç½®(ç§’)
        self.latency_config = {
            AIProvider.LOCAL_PHI: 0.05,  # 50ms
            AIProvider.OPENAI_GPT5_NANO: 0.8,  # è¶…å¿«é€Ÿå“åº”
            AIProvider.OPENAI_GPT5: 2.0,
            AIProvider.CLAUDE_SONNET_4: 1.5,
        }

        logger.info("ğŸ¤– AI Orchestrator initialized")

    async def _lazy_load_clients(self):
        """å»¶è¿ŸåŠ è½½AIå®¢æˆ·ç«¯"""
        if not self.openai_client and settings.OPENAI_API_KEY:
            import openai
            self.openai_client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            logger.info("âœ… OpenAI client loaded")

        if not self.anthropic_client and settings.ANTHROPIC_API_KEY:
            import anthropic
            self.anthropic_client = anthropic.AsyncAnthropic(
                api_key=settings.ANTHROPIC_API_KEY
            )
            logger.info("âœ… Anthropic client loaded")

        if not self.local_model and settings.USE_LOCAL_MODEL:
            # æœ¬åœ°æ¨¡å‹åŠ è½½å°†åœ¨å•ç‹¬çš„æ¨¡å—ä¸­å®ç°
            logger.info("â³ Local model loading deferred")

    async def classify_intent(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict]] = None
    ) -> IntentClassification:
        """
        æ„å›¾åˆ†ç±»
        ä½¿ç”¨ç®€å•è§„åˆ™æˆ–è½»é‡æ¨¡å‹è¿›è¡Œå¿«é€Ÿæ„å›¾è¯†åˆ«

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²

        Returns:
            IntentClassification: æ„å›¾åˆ†ç±»ç»“æœ
        """
        message_lower = user_message.lower()

        # ç®€å•è§„åˆ™åˆ†ç±»(ç”Ÿäº§ç¯å¢ƒå¯ç”¨å°æ¨¡å‹)
        if any(word in message_lower for word in ["ä½ å¥½", "hi", "hello", "æ—©", "æ™šä¸Šå¥½"]):
            return IntentClassification(
                intent=IntentType.GREETING,
                confidence=0.95
            )

        if any(word in message_lower for word in ["å¥½çš„", "å—¯", "æ˜¯çš„", "å¯¹", "ok"]):
            return IntentClassification(
                intent=IntentType.CONFIRMATION,
                confidence=0.9
            )

        if any(word in message_lower for word in ["ç¡çœ ", "å¿ƒç‡", "æ•°æ®", "æŸ¥çœ‹"]):
            return IntentClassification(
                intent=IntentType.DATA_QUERY,
                confidence=0.85,
                requires_tools=True
            )

        if any(word in message_lower for word in ["å»ºè®®", "æ€ä¹ˆ", "å¦‚ä½•", "å¸®æˆ‘"]):
            return IntentClassification(
                intent=IntentType.ADVICE_REQUEST,
                confidence=0.8,
                requires_rag=True
            )

        if any(word in message_lower for word in ["ç„¦è™‘", "å‹åŠ›", "ç´¯", "ç–²æƒ«", "éš¾å—"]):
            return IntentClassification(
                intent=IntentType.EMOTIONAL_SUPPORT,
                confidence=0.85,
                requires_empathy=True
            )

        if any(word in message_lower for word in ["åˆ†æ", "è¯„ä¼°", "è¯Šæ–­", "æ–¹æ¡ˆ"]):
            return IntentClassification(
                intent=IntentType.COMPLEX_ANALYSIS,
                confidence=0.75,
                requires_tools=True,
                requires_rag=True
            )

        # é»˜è®¤: ç®€å•å’¨è¯¢
        return IntentClassification(
            intent=IntentType.ADVICE_REQUEST,
            confidence=0.6
        )

    def _calculate_complexity(
        self,
        intent: IntentClassification,
        context_length: int,
        user_profile: Optional[Dict] = None
    ) -> int:
        """
        è®¡ç®—è¯·æ±‚å¤æ‚åº¦(1-10)

        Args:
            intent: æ„å›¾åˆ†ç±»
            context_length: ä¸Šä¸‹æ–‡é•¿åº¦(å­—ç¬¦æ•°)
            user_profile: ç”¨æˆ·ç”»åƒ

        Returns:
            int: å¤æ‚åº¦åˆ†æ•°(1-10)
        """
        complexity = 0

        # åŸºäºæ„å›¾çš„åŸºç¡€åˆ†æ•°
        intent_base_score = {
            IntentType.GREETING: 1,
            IntentType.CONFIRMATION: 1,
            IntentType.DATA_QUERY: 3,
            IntentType.ADVICE_REQUEST: 5,
            IntentType.EMOTIONAL_SUPPORT: 6,
            IntentType.COMPLEX_ANALYSIS: 8,
            IntentType.HEALTH_DIAGNOSIS: 9,
        }
        complexity += intent_base_score.get(intent.intent, 5)

        # ä¸Šä¸‹æ–‡é•¿åº¦åŠ åˆ†
        if context_length > 1000:
            complexity += 2
        elif context_length > 500:
            complexity += 1

        # éœ€è¦å·¥å…·è°ƒç”¨åŠ åˆ†
        if intent.requires_tools:
            complexity += 1

        # éœ€è¦RAGåŠ åˆ†
        if intent.requires_rag:
            complexity += 1

        return min(complexity, 10)  # ä¸Šé™10

    async def route_request(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict]] = None,
        user_profile: Optional[Dict] = None,
        force_provider: Optional[AIProvider] = None
    ) -> RoutingDecision:
        """
        æ™ºèƒ½è·¯ç”±å†³ç­–

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²
            user_profile: ç”¨æˆ·ç”»åƒ
            force_provider: å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šæä¾›å•†(ç”¨äºæµ‹è¯•)

        Returns:
            RoutingDecision: è·¯ç”±å†³ç­–
        """
        # å¦‚æœå¼ºåˆ¶æŒ‡å®šæä¾›å•†,ç›´æ¥è¿”å›
        if force_provider:
            return RoutingDecision(
                provider=force_provider,
                complexity=5,
                estimated_cost=self.cost_config[force_provider] * 2,  # å‡è®¾2K tokens
                estimated_latency=self.latency_config[force_provider],
                reason="å¼ºåˆ¶æŒ‡å®š"
            )

        # 1. æ„å›¾åˆ†ç±»
        intent = await self.classify_intent(user_message, conversation_history)

        # 2. è®¡ç®—å¤æ‚åº¦
        context_length = len(user_message)
        if conversation_history:
            context_length += sum(len(msg.get("content", "")) for msg in conversation_history)

        complexity = self._calculate_complexity(intent, context_length, user_profile)

        # 3. è·¯ç”±å†³ç­–
        provider: AIProvider
        reason: str

        if not settings.AI_COST_OPTIMIZATION:
            # ä¸å¯ç”¨æˆæœ¬ä¼˜åŒ–: å…¨éƒ¨ä½¿ç”¨GPT-5æ——èˆ°
            provider = AIProvider.OPENAI_GPT5
            reason = "æˆæœ¬ä¼˜åŒ–æœªå¯ç”¨,ä½¿ç”¨æœ€å¼ºæ¨¡å‹"

        elif complexity < settings.AI_ROUTE_LOCAL_THRESHOLD:
            # ä½å¤æ‚åº¦: ä½¿ç”¨æœ¬åœ°æ¨¡å‹
            provider = AIProvider.LOCAL_PHI
            reason = f"ä½å¤æ‚åº¦({complexity}),ä½¿ç”¨æœ¬åœ°æ¨¡å‹"

        elif complexity < settings.AI_ROUTE_MINI_THRESHOLD:
            # ä¸­ç­‰å¤æ‚åº¦: ä½¿ç”¨GPT-5 Nano(è¶…å¿«é€Ÿã€è¶…ç»æµ)
            provider = AIProvider.OPENAI_GPT5_NANO
            reason = f"ä¸­ç­‰å¤æ‚åº¦({complexity}),ä½¿ç”¨GPT-5 Nano"

        elif intent.requires_empathy:
            # éœ€è¦æƒ…æ„Ÿç†è§£: ä½¿ç”¨Claude Sonnet 4(æƒ…æ„Ÿèƒ½åŠ›æœ€å¼º)
            provider = AIProvider.CLAUDE_SONNET_4
            reason = f"éœ€è¦æƒ…æ„Ÿç†è§£,ä½¿ç”¨Claude Sonnet 4"

        else:
            # é«˜å¤æ‚åº¦: ä½¿ç”¨GPT-5æ——èˆ°
            provider = AIProvider.OPENAI_GPT5
            reason = f"é«˜å¤æ‚åº¦({complexity}),ä½¿ç”¨GPT-5"

        # è®¡ç®—é¢„ä¼°æˆæœ¬å’Œå»¶è¿Ÿ
        estimated_tokens = max(len(user_message) / 4, 100)  # ç²—ç•¥ä¼°ç®—
        estimated_cost = self.cost_config[provider] * (estimated_tokens / 1000)
        estimated_latency = self.latency_config[provider]

        decision = RoutingDecision(
            provider=provider,
            complexity=complexity,
            estimated_cost=estimated_cost,
            estimated_latency=estimated_latency,
            reason=reason,
            intent=intent
        )

        logger.info(
            f"ğŸ¯ Routing: {provider.value} | "
            f"Intent: {intent.intent.value} | "
            f"Complexity: {complexity} | "
            f"Cost: ${estimated_cost:.6f}"
        )

        return decision

    async def generate_response(
        self,
        provider: AIProvider,
        messages: List[Dict[str, str]] = None,
        system_prompt: Optional[str] = None,
        user_message: Optional[str] = None,
        conversation_history: Optional[List[Dict]] = None,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        tools: Optional[List[Dict]] = None
    ) -> AIResponse:
        """
        ç”ŸæˆAIå“åº”
        ç»Ÿä¸€çš„å“åº”ç”Ÿæˆæ¥å£,æ”¯æŒå¤šä¸ªAIæä¾›å•†

        Args:
            provider: AIæä¾›å•†
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨(å¦‚æœæä¾›,ä¼˜å…ˆä½¿ç”¨)
            system_prompt: ç³»ç»Ÿæç¤º
            user_message: ç”¨æˆ·æ¶ˆæ¯(å¦‚æœmessagesä¸ºNone,ä»æ­¤æ„å»º)
            conversation_history: ä¼šè¯å†å²
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            tools: å·¥å…·åˆ—è¡¨(ç”¨äºMCP)

        Returns:
            AIResponse: AIç”Ÿæˆçš„å“åº”å¯¹è±¡
        """
        await self._lazy_load_clients()

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        if messages is None:
            messages = []
            if conversation_history:
                messages.extend(conversation_history)
            if user_message:
                messages.append({"role": "user", "content": user_message})

        if provider == AIProvider.LOCAL_PHI:
            content = await self._generate_local(messages, system_prompt, max_tokens)
            return AIResponse(content=content, tokens_used=None)

        elif provider in [AIProvider.OPENAI_GPT5, AIProvider.OPENAI_GPT5_NANO]:
            content, tokens = await self._generate_openai(provider, messages, system_prompt, max_tokens, temperature)
            return AIResponse(content=content, tokens_used=tokens)

        elif provider == AIProvider.CLAUDE_SONNET_4:
            content, tokens = await self._generate_claude(messages, system_prompt, max_tokens, temperature, tools)
            return AIResponse(content=content, tokens_used=tokens)

        else:
            raise ValueError(f"Unsupported provider: {provider}")

    async def _generate_local(
        self,
        messages: List[Dict],
        system_prompt: Optional[str],
        max_tokens: int
    ) -> str:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆ"""
        # TODO: å®ç°æœ¬åœ°æ¨¡å‹æ¨ç†
        logger.warning("âš ï¸  Local model not implemented yet, using mock response")
        return "è¿™æ˜¯æœ¬åœ°æ¨¡å‹çš„æ¨¡æ‹Ÿå“åº”ã€‚å®é™…å®ç°ä¸­ä¼šåŠ è½½Phi-3.5æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚"

    async def _generate_openai(
        self,
        provider: AIProvider,
        messages: List[Dict],
        system_prompt: Optional[str],
        max_tokens: int,
        temperature: float
    ) -> Tuple[str, int]:
        """ä½¿ç”¨OpenAIç”Ÿæˆ,è¿”å›(content, tokens)"""
        if not self.openai_client:
            raise RuntimeError("OpenAI client not initialized")

        model = settings.OPENAI_MODEL_MAIN if provider == AIProvider.OPENAI_GPT5 else settings.OPENAI_MODEL_MINI

        # æ·»åŠ ç³»ç»Ÿæç¤º
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)

        try:
            # GPT-5ç³»åˆ—ä½¿ç”¨max_completion_tokensï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨max_tokens
            token_param = "max_completion_tokens" if model.startswith("gpt-5") else "max_tokens"

            response = await self.openai_client.chat.completions.create(
                model=model,
                messages=full_messages,
                **{token_param: max_tokens},
                temperature=temperature
            )

            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else None

            return content, tokens_used

        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            raise

    async def _generate_claude(
        self,
        messages: List[Dict],
        system_prompt: Optional[str],
        max_tokens: int,
        temperature: float,
        tools: Optional[List[Dict]]
    ) -> Tuple[str, int]:
        """ä½¿ç”¨Claudeç”Ÿæˆ,è¿”å›(content, tokens)"""
        if not self.anthropic_client:
            raise RuntimeError("Anthropic client not initialized")

        try:
            response = await self.anthropic_client.messages.create(
                model=settings.ANTHROPIC_MODEL,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt or "",
                messages=messages,
                tools=tools or []
            )

            # å¤„ç†å·¥å…·è°ƒç”¨
            if response.stop_reason == "tool_use":
                # TODO: å¤„ç†MCPå·¥å…·è°ƒç”¨
                logger.info("ğŸ”§ Tool use detected")

            content = response.content[0].text
            tokens_used = response.usage.input_tokens + response.usage.output_tokens if response.usage else None

            return content, tokens_used

        except Exception as e:
            logger.error(f"Anthropic API error: {e}")
            raise


# å…¨å±€å•ä¾‹
orchestrator = AIOrchestrator()
