"""
AI Orchestrator - æ™ºèƒ½è·¯ç”±æ ¸å¿ƒ
æ ¹æ®è¯·æ±‚å¤æ‚åº¦å’Œæ„å›¾,è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„AIæ¨¡å‹
å®ç°æˆæœ¬ä¼˜åŒ–å’Œæ€§èƒ½ä¼˜åŒ–çš„å¹³è¡¡
"""

from enum import Enum
from typing import Optional, Tuple, List, Dict, Any
from dataclasses import dataclass
import asyncio
from loguru import logger

from app.core.config import settings


class AIProvider(str, Enum):
    """AIæä¾›å•†æšä¸¾"""
    LOCAL_PHI = "phi-3.5"
    OPENAI_GPT5 = "gpt-5"  # GPT-5 æ——èˆ°æ¨¡å‹
    OPENAI_GPT5_NANO = "gpt-5-nano-2025-08-07"  # GPT-5 Nano æœ€æ–°ç‰ˆæœ¬
    CLAUDE_SONNET_4 = "claude-sonnet-4-20250514"  # Claude Sonnet 4 æœ€æ–°ç‰ˆæœ¬


class IntentType(str, Enum):
    """æ„å›¾ç±»å‹"""
    GREETING = "greeting"  # é—®å€™
    CONFIRMATION = "confirmation"  # ç¡®è®¤
    DATA_QUERY = "data_query"  # æ•°æ®æŸ¥è¯¢
    ADVICE_REQUEST = "advice_request"  # å»ºè®®è¯·æ±‚
    EMOTIONAL_SUPPORT = "emotional_support"  # æƒ…æ„Ÿæ”¯æŒ
    COMPLEX_ANALYSIS = "complex_analysis"  # å¤æ‚åˆ†æ
    HEALTH_DIAGNOSIS = "health_diagnosis"  # å¥åº·è¯Šæ–­


@dataclass
class IntentClassification:
    """æ„å›¾åˆ†ç±»ç»“æœ"""
    intent: IntentType
    confidence: float
    requires_empathy: bool = False  # æ˜¯å¦éœ€è¦æƒ…æ„Ÿç†è§£
    requires_tools: bool = False  # æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
    requires_rag: bool = False  # æ˜¯å¦éœ€è¦RAGå¢å¼º


@dataclass
class RoutingDecision:
    """è·¯ç”±å†³ç­–ç»“æœ"""
    provider: AIProvider
    complexity: int  # 1-10
    estimated_cost: float  # é¢„ä¼°æˆæœ¬(ç¾å…ƒ)
    estimated_latency: float  # é¢„ä¼°å»¶è¿Ÿ(ç§’)
    reason: str  # è·¯ç”±åŸå› 
    intent: IntentClassification  # æ„å›¾åˆ†ç±»ç»“æœ


@dataclass
class AIResponse:
    """AIå“åº”ç»“æœ"""
    content: str  # å“åº”å†…å®¹
    tokens_used: Optional[int] = None  # ä½¿ç”¨çš„tokenæ•°
    finish_reason: Optional[str] = None  # å®ŒæˆåŸå› 


class AIOrchestrator:
    """
    AIç¼–æ’å™¨
    è´Ÿè´£æ™ºèƒ½è·¯ç”±ã€è¯·æ±‚åˆ†å‘ã€å“åº”èšåˆ
    """

    def __init__(self):
        """åˆå§‹åŒ–ç¼–æ’å™¨"""
        self.local_model = None  # å»¶è¿ŸåŠ è½½
        self.openai_client = None
        self.anthropic_client = None

        # æˆæœ¬é…ç½®(æ¯1K tokens) - åŸºäº2025å¹´æœ€æ–°å®šä»·
        self.cost_config = {
            AIProvider.LOCAL_PHI: 0.0,
            AIProvider.OPENAI_GPT5_NANO: 0.0002,  # $0.2/1M tokens (è¶…ç»æµ)
            AIProvider.OPENAI_GPT5: 0.005,  # $5/1M tokens (æ——èˆ°çº§)
            AIProvider.CLAUDE_SONNET_4: 0.003,  # $3/1M tokens
        }

        # å»¶è¿Ÿé…ç½®(ç§’)
        self.latency_config = {
            AIProvider.LOCAL_PHI: 0.05,  # 50ms
            AIProvider.OPENAI_GPT5_NANO: 0.8,  # è¶…å¿«é€Ÿå“åº”
            AIProvider.OPENAI_GPT5: 2.0,
            AIProvider.CLAUDE_SONNET_4: 1.5,
        }

        logger.info("ğŸ¤– AI Orchestrator initialized")

    async def _lazy_load_clients(self):
        """å»¶è¿ŸåŠ è½½AIå®¢æˆ·ç«¯"""
        if not self.openai_client and settings.OPENAI_API_KEY:
            import openai
            self.openai_client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            logger.info("âœ… OpenAI client loaded")

        if not self.anthropic_client and settings.ANTHROPIC_API_KEY:
            import anthropic
            self.anthropic_client = anthropic.AsyncAnthropic(
                api_key=settings.ANTHROPIC_API_KEY
            )
            logger.info("âœ… Anthropic client loaded")

        if not self.local_model and settings.USE_LOCAL_MODEL:
            # æœ¬åœ°æ¨¡å‹åŠ è½½å°†åœ¨å•ç‹¬çš„æ¨¡å—ä¸­å®ç°
            logger.info("â³ Local model loading deferred")

    async def classify_intent(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict]] = None
    ) -> IntentClassification:
        """
        æ„å›¾åˆ†ç±» (å‡çº§ç‰ˆ)
        ä½¿ç”¨æ··åˆç­–ç•¥: è§„åˆ™åŒ¹é… + Sentence-Transformersè¯­ä¹‰ç›¸ä¼¼åº¦

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²

        Returns:
            IntentClassification: æ„å›¾åˆ†ç±»ç»“æœ
        """
        from app.ai.intent_classifier import get_intent_classifier

        classifier = get_intent_classifier()

        # è°ƒç”¨æ–°çš„åˆ†ç±»å™¨
        result = await classifier.classify(
            message=user_message,
            conversation_history=conversation_history
        )

        # è®°å½•åˆ†ç±»ç»“æœ (ç”¨äºç›‘æ§)
        logger.info(
            f"ğŸ¯ Intent: {result.intent.value} | "
            f"Confidence: {result.confidence:.2f}"
        )

        return result

    async def _calculate_complexity(
        self,
        intent: IntentClassification,
        user_message: str,
        conversation_history: Optional[List[Dict]] = None,
        user_profile: Optional[Dict] = None,
        user_id: Optional[str] = None
    ) -> int:
        """
        è®¡ç®—è¯·æ±‚å¤æ‚åº¦(1-10) - å‡çº§ç‰ˆ
        ä½¿ç”¨ComplexityAnalyzerè¿›è¡Œæ™ºèƒ½åˆ†æ

        Args:
            intent: æ„å›¾åˆ†ç±»
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²
            user_profile: ç”¨æˆ·ç”»åƒ
            user_id: ç”¨æˆ·ID

        Returns:
            int: å¤æ‚åº¦åˆ†æ•°(1-10)
        """
        from app.ai.complexity_analyzer import get_complexity_analyzer

        analyzer = get_complexity_analyzer()

        # ä½¿ç”¨ComplexityAnalyzerè¿›è¡Œæ™ºèƒ½åˆ†æ
        factors = await analyzer.analyze_complexity(
            intent=intent,
            user_message=user_message,
            conversation_history=conversation_history,
            user_profile=user_profile,
            user_id=user_id
        )

        return factors.total_score

    async def route_request(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict]] = None,
        user_profile: Optional[Dict] = None,
        force_provider: Optional[AIProvider] = None,
        user_id: Optional[str] = None
    ) -> RoutingDecision:
        """
        æ™ºèƒ½è·¯ç”±å†³ç­–

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²
            user_profile: ç”¨æˆ·ç”»åƒ
            force_provider: å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šæä¾›å•†(ç”¨äºæµ‹è¯•)

        Returns:
            RoutingDecision: è·¯ç”±å†³ç­–
        """
        # å¦‚æœå¼ºåˆ¶æŒ‡å®šæä¾›å•†,ç›´æ¥è¿”å›
        if force_provider:
            return RoutingDecision(
                provider=force_provider,
                complexity=5,
                estimated_cost=self.cost_config[force_provider] * 2,  # å‡è®¾2K tokens
                estimated_latency=self.latency_config[force_provider],
                reason="å¼ºåˆ¶æŒ‡å®š"
            )

        # 1. æ„å›¾åˆ†ç±»
        intent = await self.classify_intent(user_message, conversation_history)

        # 2. è®¡ç®—å¤æ‚åº¦ (ä½¿ç”¨å‡çº§ç‰ˆComplexityAnalyzer)
        complexity = await self._calculate_complexity(
            intent=intent,
            user_message=user_message,
            conversation_history=conversation_history,
            user_profile=user_profile,
            user_id=user_id
        )

        # 3. è·¯ç”±å†³ç­–
        provider: AIProvider
        reason: str

        if not settings.AI_COST_OPTIMIZATION:
            # ä¸å¯ç”¨æˆæœ¬ä¼˜åŒ–: å…¨éƒ¨ä½¿ç”¨GPT-5æ——èˆ°
            provider = AIProvider.OPENAI_GPT5
            reason = "æˆæœ¬ä¼˜åŒ–æœªå¯ç”¨,ä½¿ç”¨æœ€å¼ºæ¨¡å‹"

        elif complexity < settings.AI_ROUTE_LOCAL_THRESHOLD:
            # ä½å¤æ‚åº¦: ä½¿ç”¨æœ¬åœ°æ¨¡å‹
            provider = AIProvider.LOCAL_PHI
            reason = f"ä½å¤æ‚åº¦({complexity}),ä½¿ç”¨æœ¬åœ°æ¨¡å‹"

        elif complexity < settings.AI_ROUTE_MINI_THRESHOLD:
            # ä¸­ç­‰å¤æ‚åº¦: ä½¿ç”¨GPT-5 Nano(è¶…å¿«é€Ÿã€è¶…ç»æµ)
            provider = AIProvider.OPENAI_GPT5_NANO
            reason = f"ä¸­ç­‰å¤æ‚åº¦({complexity}),ä½¿ç”¨GPT-5 Nano"

        elif intent.requires_empathy:
            # éœ€è¦æƒ…æ„Ÿç†è§£: ä½¿ç”¨Claude Sonnet 4(æƒ…æ„Ÿèƒ½åŠ›æœ€å¼º)
            provider = AIProvider.CLAUDE_SONNET_4
            reason = f"éœ€è¦æƒ…æ„Ÿç†è§£,ä½¿ç”¨Claude Sonnet 4"

        else:
            # é«˜å¤æ‚åº¦: ä½¿ç”¨GPT-5æ——èˆ°
            provider = AIProvider.OPENAI_GPT5
            reason = f"é«˜å¤æ‚åº¦({complexity}),ä½¿ç”¨GPT-5"

        # è®¡ç®—é¢„ä¼°æˆæœ¬å’Œå»¶è¿Ÿ
        estimated_tokens = max(len(user_message) / 4, 100)  # ç²—ç•¥ä¼°ç®—
        estimated_cost = self.cost_config[provider] * (estimated_tokens / 1000)
        estimated_latency = self.latency_config[provider]

        decision = RoutingDecision(
            provider=provider,
            complexity=complexity,
            estimated_cost=estimated_cost,
            estimated_latency=estimated_latency,
            reason=reason,
            intent=intent
        )

        logger.info(
            f"ğŸ¯ Routing: {provider.value} | "
            f"Intent: {intent.intent.value} | "
            f"Complexity: {complexity} | "
            f"Cost: ${estimated_cost:.6f}"
        )

        return decision

    async def generate_response(
        self,
        provider: AIProvider,
        messages: List[Dict[str, str]] = None,
        system_prompt: Optional[str] = None,
        user_message: Optional[str] = None,
        conversation_history: Optional[List[Dict]] = None,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        tools: Optional[List[Dict]] = None,
        user_id: Optional["UUID"] = None,
        db: Optional["AsyncSession"] = None
    ) -> AIResponse:
        """
        ç”ŸæˆAIå“åº”
        ç»Ÿä¸€çš„å“åº”ç”Ÿæˆæ¥å£,æ”¯æŒå¤šä¸ªAIæä¾›å•†

        Args:
            provider: AIæä¾›å•†
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨(å¦‚æœæä¾›,ä¼˜å…ˆä½¿ç”¨)
            system_prompt: ç³»ç»Ÿæç¤º
            user_message: ç”¨æˆ·æ¶ˆæ¯(å¦‚æœmessagesä¸ºNone,ä»æ­¤æ„å»º)
            conversation_history: ä¼šè¯å†å²
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            tools: å·¥å…·åˆ—è¡¨(ç”¨äºMCP)

        Returns:
            AIResponse: AIç”Ÿæˆçš„å“åº”å¯¹è±¡
        """
        await self._lazy_load_clients()

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        if messages is None:
            messages = []
            if conversation_history:
                messages.extend(conversation_history)
            if user_message:
                messages.append({"role": "user", "content": user_message})

        if provider == AIProvider.LOCAL_PHI:
            content = await self._generate_local(messages, system_prompt, max_tokens)
            return AIResponse(content=content, tokens_used=None)

        elif provider in [AIProvider.OPENAI_GPT5, AIProvider.OPENAI_GPT5_NANO]:
            content, tokens = await self._generate_openai(provider, messages, system_prompt, max_tokens, temperature)
            return AIResponse(content=content, tokens_used=tokens)

        elif provider == AIProvider.CLAUDE_SONNET_4:
            content, tokens = await self._generate_claude(
                messages, system_prompt, max_tokens, temperature, tools, user_id, db
            )
            return AIResponse(content=content, tokens_used=tokens)

        else:
            raise ValueError(f"Unsupported provider: {provider}")

    async def _generate_local(
        self,
        messages: List[Dict],
        system_prompt: Optional[str],
        max_tokens: int
    ) -> str:
        """ä½¿ç”¨æœ¬åœ°Phi-3.5æ¨¡å‹ç”Ÿæˆ"""
        from app.ai.local_models import get_local_model_manager

        local_manager = get_local_model_manager()

        # æ„å»ºå®Œæ•´æç¤ºè¯
        prompt_parts = []

        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system_prompt:
            prompt_parts.append(f"ç³»ç»ŸæŒ‡ä»¤: {system_prompt}\n")

        # æ·»åŠ å¯¹è¯å†å²
        for msg in messages:
            role = msg["role"]
            content = msg["content"]

            if role == "user":
                prompt_parts.append(f"ç”¨æˆ·: {content}")
            elif role == "assistant":
                prompt_parts.append(f"åŠ©æ‰‹: {content}")

        full_prompt = "\n\n".join(prompt_parts)

        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆ
        response = await local_manager.generate(
            prompt=full_prompt,
            max_new_tokens=max_tokens,
            temperature=0.7,
            top_p=0.9
        )

        return response

    async def _generate_openai(
        self,
        provider: AIProvider,
        messages: List[Dict],
        system_prompt: Optional[str],
        max_tokens: int,
        temperature: float
    ) -> Tuple[str, int]:
        """ä½¿ç”¨OpenAIç”Ÿæˆ,è¿”å›(content, tokens)"""
        if not self.openai_client:
            raise RuntimeError("OpenAI client not initialized")

        model = settings.OPENAI_MODEL_MAIN if provider == AIProvider.OPENAI_GPT5 else settings.OPENAI_MODEL_MINI

        # æ·»åŠ ç³»ç»Ÿæç¤º
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)

        try:
            # GPT-5ç³»åˆ—ä½¿ç”¨max_completion_tokensï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨max_tokens
            token_param = "max_completion_tokens" if model.startswith("gpt-5") else "max_tokens"

            response = await self.openai_client.chat.completions.create(
                model=model,
                messages=full_messages,
                **{token_param: max_tokens},
                temperature=temperature
            )

            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else None

            return content, tokens_used

        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            raise

    async def _generate_claude(
        self,
        messages: List[Dict],
        system_prompt: Optional[str],
        max_tokens: int,
        temperature: float,
        tools: Optional[List[Dict]],
        user_id: Optional["UUID"] = None,
        db: Optional["AsyncSession"] = None
    ) -> Tuple[str, int]:
        """
        ä½¿ç”¨Claudeç”Ÿæˆ,æ”¯æŒMCPå·¥å…·è°ƒç”¨

        å¤„ç†æµç¨‹ï¼š
        1. è°ƒç”¨Claude API (ä¼ å…¥tools)
        2. å¦‚æœå“åº”åŒ…å«tool_useï¼Œæ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœç»™Claude
        3. ClaudeåŸºäºå·¥å…·ç»“æœç»§ç»­ç”Ÿæˆæœ€ç»ˆå“åº”
        4. é€’å½’å¤„ç†ç›´åˆ°è·å¾—æœ€ç»ˆæ–‡æœ¬å“åº”
        """
        if not self.anthropic_client:
            raise RuntimeError("Anthropic client not initialized")

        try:
            # è°ƒç”¨Claude API
            response = await self.anthropic_client.messages.create(
                model=settings.ANTHROPIC_MODEL,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt or "",
                messages=messages,
                tools=tools or []
            )

            # å¤„ç†å·¥å…·è°ƒç”¨
            if response.stop_reason == "tool_use":
                logger.info("ğŸ”§ Tool use detected, processing...")

                # å¯¼å…¥MCPå·¥å…·æ‰§è¡Œå™¨
                from app.mcp import execute_tool
                import json

                # æ”¶é›†æ‰€æœ‰å·¥å…·è°ƒç”¨ç»“æœ
                tool_results = []

                for content_block in response.content:
                    if content_block.type == "tool_use":
                        tool_name = content_block.name
                        tool_input = content_block.input
                        tool_use_id = content_block.id

                        logger.info(f"   Executing tool: {tool_name}")
                        logger.debug(f"   Input: {tool_input}")

                        try:
                            # æ‰§è¡Œå·¥å…·
                            result = await execute_tool(
                                tool_name=tool_name,
                                tool_input=tool_input,
                                user_id=user_id,
                                db=db
                            )

                            logger.info(f"   âœ… Tool executed: {tool_name}")

                            # æ·»åŠ å·¥å…·ç»“æœ
                            tool_results.append({
                                "type": "tool_result",
                                "tool_use_id": tool_use_id,
                                "content": json.dumps(result, ensure_ascii=False)
                            })

                        except Exception as e:
                            logger.error(f"   âŒ Tool execution failed: {tool_name} - {e}")

                            # è¿”å›é”™è¯¯ä¿¡æ¯ç»™Claude
                            tool_results.append({
                                "type": "tool_result",
                                "tool_use_id": tool_use_id,
                                "content": json.dumps({
                                    "error": str(e),
                                    "tool": tool_name
                                }, ensure_ascii=False),
                                "is_error": True
                            })

                # å°†å·¥å…·è°ƒç”¨å’Œç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
                messages.append({
                    "role": "assistant",
                    "content": response.content
                })

                messages.append({
                    "role": "user",
                    "content": tool_results
                })

                # é€’å½’è°ƒç”¨ï¼Œè®©ClaudeåŸºäºå·¥å…·ç»“æœç»§ç»­ç”Ÿæˆ
                logger.info("ğŸ”„ Calling Claude with tool results...")
                return await self._generate_claude(
                    messages=messages,
                    system_prompt=system_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    tools=tools,
                    user_id=user_id,
                    db=db
                )

            # æå–æ–‡æœ¬å†…å®¹
            content = ""
            for block in response.content:
                if block.type == "text":
                    content += block.text

            tokens_used = response.usage.input_tokens + response.usage.output_tokens if response.usage else None

            return content, tokens_used

        except Exception as e:
            logger.error(f"Anthropic API error: {e}")
            raise


# å…¨å±€å•ä¾‹
orchestrator = AIOrchestrator()
