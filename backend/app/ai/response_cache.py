"""
AIå“åº”ç¼“å­˜ç®¡ç†å™¨
ä¸‰å±‚ç¼“å­˜æ¶æ„ï¼šL1(Redisç²¾ç¡®åŒ¹é…) + L2(Qdrantè¯­ä¹‰ç›¸ä¼¼) + L3(çŸ¥è¯†åº“é¢„ç­”æ¡ˆ)

ç›®æ ‡ï¼š
- é™ä½AIè°ƒç”¨æˆæœ¬ 20-30%
- æå‡å“åº”é€Ÿåº¦ 50-200å€ï¼ˆç¼“å­˜å‘½ä¸­æ—¶ï¼‰
- æ”¹å–„ç”¨æˆ·ä½“éªŒ
"""

import hashlib
import json
import asyncio
from datetime import datetime
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass, asdict
from loguru import logger

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

from app.core.config import settings


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    query: str  # ç”¨æˆ·é—®é¢˜
    response: str  # AIå›ç­”
    provider: str  # ä½¿ç”¨çš„AIæä¾›å•†
    intent: str  # æ„å›¾ç±»å‹
    complexity: int  # å¤æ‚åº¦(1-10)
    tokens_used: int  # tokenä½¿ç”¨é‡
    cached_at: str  # ç¼“å­˜æ—¶é—´ (ISOæ ¼å¼)
    hit_count: int = 0  # å‘½ä¸­æ¬¡æ•°
    user_id: Optional[str] = None  # ç”¨æˆ·ID

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> "CacheEntry":
        """ä»å­—å…¸åˆ›å»º"""
        return cls(**data)


@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡"""
    total_requests: int = 0
    l1_hits: int = 0  # Redisç²¾ç¡®åŒ¹é…å‘½ä¸­
    l2_hits: int = 0  # Qdrantè¯­ä¹‰ç›¸ä¼¼å‘½ä¸­
    l3_hits: int = 0  # çŸ¥è¯†åº“é¢„ç­”æ¡ˆå‘½ä¸­
    cache_misses: int = 0  # æœªå‘½ä¸­
    total_cost_saved_usd: float = 0.0  # èŠ‚çœæˆæœ¬
    total_latency_saved_ms: float = 0.0  # èŠ‚çœå»¶è¿Ÿ
    avg_response_time_cached_ms: float = 0.0  # ç¼“å­˜å‘½ä¸­å¹³å‡å“åº”æ—¶é—´
    avg_response_time_uncached_ms: float = 0.0  # æœªç¼“å­˜å¹³å‡å“åº”æ—¶é—´


class ResponseCacheManager:
    """
    AIå“åº”ç¼“å­˜ç®¡ç†å™¨

    ä¸‰å±‚ç¼“å­˜ç­–ç•¥:
    - L1 (Redis): ç²¾ç¡®åŒ¹é…ï¼Œæœ€å¿« (<5ms)ï¼Œå‘½ä¸­ç‡15-20%
    - L2 (Qdrant): è¯­ä¹‰ç›¸ä¼¼ï¼Œå¿« (~30ms)ï¼Œå‘½ä¸­ç‡8-12%
    - L3 (Qdrant): çŸ¥è¯†åº“é¢„ç­”æ¡ˆï¼Œå¿« (~30ms)ï¼Œå‘½ä¸­ç‡2-5%

    æ€»ä½“é¢„æœŸå‘½ä¸­ç‡: 25-35%
    """

    # æˆæœ¬ä¼°ç®—ï¼ˆæ¯1K tokensç¾å…ƒï¼‰
    COST_PER_1K_TOKENS = {
        "gpt-5": 0.03,
        "gpt-5-nano-2025-08-07": 0.01,
        "claude-sonnet-4-20250514": 0.015,
        "phi-3.5": 0.0,  # æœ¬åœ°æ¨¡å‹æ— æˆæœ¬
    }

    # å»¶è¿Ÿä¼°ç®—ï¼ˆæ¯«ç§’ï¼‰
    LATENCY_ESTIMATE_MS = {
        "gpt-5": 3000,
        "gpt-5-nano-2025-08-07": 1500,
        "claude-sonnet-4-20250514": 2500,
        "phi-3.5": 500,
    }

    def __init__(self):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        self.redis_manager = None
        self.qdrant_client: Optional[QdrantClient] = None
        self.sentence_transformer = None

        # ç»Ÿè®¡æ•°æ®
        self.stats = CacheStats()

        # åˆå§‹åŒ–æ ‡å¿—
        self._initialized = False
        self._init_lock = asyncio.Lock()

        logger.info("ğŸ“¦ ResponseCacheManager created")

    async def initialize(self):
        """
        åˆå§‹åŒ–æ‰€æœ‰ä¾èµ–

        æ‡’åŠ è½½æ¨¡å¼ï¼Œä»…åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–
        """
        if self._initialized:
            return

        async with self._init_lock:
            if self._initialized:
                return

            try:
                # 1. Rediså®¢æˆ·ç«¯
                from app.core.redis_client import get_redis_manager
                self.redis_manager = await get_redis_manager()

                # 2. Qdrantå®¢æˆ·ç«¯
                self.qdrant_client = QdrantClient(
                    url=settings.QDRANT_URL,
                    api_key=settings.QDRANT_API_KEY,
                    timeout=10
                )

                # 3. å¤ç”¨æ„å›¾åˆ†ç±»å™¨çš„Sentence-Transformer
                from app.ai.intent_classifier import get_intent_classifier
                classifier = get_intent_classifier()

                # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
                if not classifier.is_loaded:
                    async with classifier.load_lock:
                        if not classifier.is_loaded:
                            await classifier._load_model()

                self.sentence_transformer = classifier.model

                # 4. ç¡®ä¿çŸ¥è¯†åº“collectionå­˜åœ¨
                await self._ensure_knowledge_base_collection()

                self._initialized = True
                logger.info("âœ… ResponseCacheManager initialized")

            except Exception as e:
                logger.error(f"âŒ ResponseCacheManager initialization failed: {e}")
                raise

    async def _ensure_knowledge_base_collection(self):
        """ç¡®ä¿çŸ¥è¯†åº“collectionå­˜åœ¨"""
        collection_name = "knowledge_base_qa"

        try:
            # æ£€æŸ¥collectionæ˜¯å¦å­˜åœ¨
            self.qdrant_client.get_collection(collection_name)
            logger.debug(f"âœ“ Qdrant collection '{collection_name}' exists")
        except:
            # ä¸å­˜åœ¨åˆ™åˆ›å»º
            self.qdrant_client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=384,  # MiniLMç»´åº¦
                    distance=Distance.COSINE
                )
            )
            logger.info(f"âœ… Created Qdrant collection '{collection_name}'")

    def _generate_cache_key(
        self,
        query: str,
        user_id: str,
        context_hash: Optional[str] = None
    ) -> str:
        """
        ç”ŸæˆRedisç¼“å­˜é”®

        æ ¼å¼: response:{user_id}:{query_hash}[:context_hash]

        Args:
            query: ç”¨æˆ·é—®é¢˜
            user_id: ç”¨æˆ·ID
            context_hash: ä¸Šä¸‹æ–‡å“ˆå¸Œï¼ˆå¯é€‰ï¼‰

        Returns:
            ç¼“å­˜é”®
        """
        # å½’ä¸€åŒ–æŸ¥è¯¢ï¼ˆå°å†™+å»é™¤é¦–å°¾ç©ºæ ¼ï¼‰
        query_normalized = query.lower().strip()

        # ç”ŸæˆæŸ¥è¯¢å“ˆå¸Œï¼ˆMD5å‰16ä½ï¼‰
        query_hash = hashlib.md5(query_normalized.encode()).hexdigest()[:16]

        if context_hash:
            return f"response:{user_id}:{query_hash}:{context_hash}"

        return f"response:{user_id}:{query_hash}"

    def _calculate_context_hash(
        self,
        conversation_history: Optional[List[Dict]]
    ) -> Optional[str]:
        """
        è®¡ç®—å¯¹è¯ä¸Šä¸‹æ–‡å“ˆå¸Œ

        ä»…ä½¿ç”¨æœ€è¿‘3è½®å¯¹è¯è®¡ç®—hash

        Args:
            conversation_history: å¯¹è¯å†å²

        Returns:
            ä¸Šä¸‹æ–‡å“ˆå¸Œï¼Œæ— å†å²è¿”å›None
        """
        if not conversation_history or len(conversation_history) == 0:
            return None

        # å–æœ€è¿‘3è½®å¯¹è¯
        recent_history = conversation_history[-3:]

        # æ‹¼æ¥å†…å®¹
        context_str = ""
        for msg in recent_history:
            role = msg.get("role", "")
            content = msg.get("content", "")
            context_str += f"{role}:{content}|"

        # ç”Ÿæˆå“ˆå¸Œ
        return hashlib.md5(context_str.encode()).hexdigest()[:8]

    async def get_cached_response(
        self,
        query: str,
        user_id: str,
        conversation_history: Optional[List[Dict]] = None,
        similarity_threshold: float = 0.92
    ) -> Optional[Tuple[CacheEntry, str]]:
        """
        è·å–ç¼“å­˜å“åº”

        æŒ‰L1 â†’ L2 â†’ L3é¡ºåºæ£€æŸ¥

        Args:
            query: ç”¨æˆ·é—®é¢˜
            user_id: ç”¨æˆ·ID
            conversation_history: å¯¹è¯å†å²
            similarity_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼(0-1)

        Returns:
            (CacheEntry, cache_layer) æˆ– None
            cache_layer: "L1" | "L2" | "L3"
        """
        # ç¡®ä¿å·²åˆå§‹åŒ–
        if not self._initialized:
            await self.initialize()

        self.stats.total_requests += 1
        start_time = datetime.now()

        # L1: Redisç²¾ç¡®åŒ¹é…
        l1_result = await self._check_l1_cache(query, user_id, conversation_history)
        if l1_result:
            self.stats.l1_hits += 1
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000
            self._update_cache_latency_stats(latency_ms, cached=True)

            logger.debug(
                f"âœ… L1 HIT | "
                f"Query: {query[:30]}... | "
                f"Latency: {latency_ms:.1f}ms"
            )
            return (l1_result, "L1")

        # L2: Qdrantè¯­ä¹‰ç›¸ä¼¼åŒ¹é…
        l2_result = await self._check_l2_cache(
            query,
            user_id,
            similarity_threshold
        )
        if l2_result:
            self.stats.l2_hits += 1
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000
            self._update_cache_latency_stats(latency_ms, cached=True)

            logger.debug(
                f"âœ… L2 HIT | "
                f"Query: {query[:30]}... | "
                f"Latency: {latency_ms:.1f}ms"
            )
            return (l2_result, "L2")

        # L3: çŸ¥è¯†åº“é¢„ç­”æ¡ˆ
        l3_result = await self._check_l3_cache(query)
        if l3_result:
            self.stats.l3_hits += 1
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000
            self._update_cache_latency_stats(latency_ms, cached=True)

            logger.debug(
                f"âœ… L3 HIT | "
                f"Query: {query[:30]}... | "
                f"Latency: {latency_ms:.1f}ms"
            )
            return (l3_result, "L3")

        # å…¨éƒ¨æœªå‘½ä¸­
        self.stats.cache_misses += 1
        logger.debug(f"âŒ CACHE MISS | Query: {query[:30]}...")

        return None

    async def _check_l1_cache(
        self,
        query: str,
        user_id: str,
        conversation_history: Optional[List[Dict]]
    ) -> Optional[CacheEntry]:
        """
        L1: Redisç²¾ç¡®åŒ¹é…æ£€æŸ¥

        æœ€å¿«ï¼Œ<5ms
        """
        try:
            # è®¡ç®—ä¸Šä¸‹æ–‡å“ˆå¸Œ
            context_hash = self._calculate_context_hash(conversation_history)

            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._generate_cache_key(query, user_id, context_hash)

            # ä»Redisè¯»å–
            cached_data = await self.redis_manager.get(cache_key)

            if cached_data:
                data = json.loads(cached_data)
                cache_entry = CacheEntry.from_dict(data)

                # æ›´æ–°å‘½ä¸­æ¬¡æ•°
                cache_entry.hit_count += 1
                await self.redis_manager.set(
                    cache_key,
                    json.dumps(cache_entry.to_dict()),
                    ttl=86400  # ä¿æŒ24å°æ—¶TTL
                )

                return cache_entry

            return None

        except Exception as e:
            logger.error(f"L1 cache check error: {e}")
            return None

    async def _check_l2_cache(
        self,
        query: str,
        user_id: str,
        threshold: float
    ) -> Optional[CacheEntry]:
        """
        L2: Qdrantè¯­ä¹‰ç›¸ä¼¼æ£€æŸ¥

        è¾ƒå¿«ï¼Œ~30ms
        """
        try:
            # ç¼–ç æŸ¥è¯¢å‘é‡
            loop = asyncio.get_event_loop()
            query_vector = await loop.run_in_executor(
                None,
                lambda: self.sentence_transformer.encode(
                    query,
                    convert_to_tensor=False
                ).tolist()
            )

            # ç”¨æˆ·ä¸“å±collectionåç§°
            collection_name = f"cache_user_{user_id}"

            # æ£€æŸ¥collectionæ˜¯å¦å­˜åœ¨
            try:
                self.qdrant_client.get_collection(collection_name)
            except:
                # collectionä¸å­˜åœ¨ï¼Œæœªå‘½ä¸­
                return None

            # Qdrantå‘é‡æœç´¢
            search_results = self.qdrant_client.search(
                collection_name=collection_name,
                query_vector=query_vector,
                limit=1,
                score_threshold=threshold
            )

            if search_results and len(search_results) > 0:
                result = search_results[0]
                if result.score >= threshold:
                    cache_entry = CacheEntry.from_dict(result.payload)
                    return cache_entry

            return None

        except Exception as e:
            logger.error(f"L2 cache check error: {e}")
            return None

    async def _check_l3_cache(self, query: str) -> Optional[CacheEntry]:
        """
        L3: çŸ¥è¯†åº“é¢„ç­”æ¡ˆæ£€æŸ¥

        å¿«ï¼Œ~30ms
        """
        try:
            # ç¼–ç æŸ¥è¯¢å‘é‡
            loop = asyncio.get_event_loop()
            query_vector = await loop.run_in_executor(
                None,
                lambda: self.sentence_transformer.encode(
                    query,
                    convert_to_tensor=False
                ).tolist()
            )

            # ä»çŸ¥è¯†åº“æœç´¢
            search_results = self.qdrant_client.search(
                collection_name="knowledge_base_qa",
                query_vector=query_vector,
                limit=1,
                score_threshold=0.88  # L3é˜ˆå€¼ç•¥ä½
            )

            if search_results and len(search_results) > 0:
                result = search_results[0]
                if result.score >= 0.88:
                    cache_entry = CacheEntry.from_dict(result.payload)
                    return cache_entry

            return None

        except Exception as e:
            logger.error(f"L3 cache check error: {e}")
            return None

    async def set_cache(
        self,
        query: str,
        response: str,
        user_id: str,
        provider: str,
        intent: str,
        complexity: int,
        tokens_used: int,
        conversation_history: Optional[List[Dict]] = None,
        ttl: int = 86400  # 24å°æ—¶
    ):
        """
        è®¾ç½®ç¼“å­˜

        åŒæ—¶å†™å…¥L1(Redis)å’ŒL2(Qdrant)

        Args:
            query: ç”¨æˆ·é—®é¢˜
            response: AIå›ç­”
            user_id: ç”¨æˆ·ID
            provider: AIæä¾›å•†
            intent: æ„å›¾ç±»å‹
            complexity: å¤æ‚åº¦
            tokens_used: tokenä½¿ç”¨é‡
            conversation_history: å¯¹è¯å†å²
            ttl: Redisç¼“å­˜TTLï¼ˆç§’ï¼‰
        """
        try:
            # ç¡®ä¿å·²åˆå§‹åŒ–
            if not self._initialized:
                await self.initialize()

            # åˆ›å»ºç¼“å­˜æ¡ç›®
            cache_entry = CacheEntry(
                query=query,
                response=response,
                provider=provider,
                intent=intent,
                complexity=complexity,
                tokens_used=tokens_used,
                cached_at=datetime.utcnow().isoformat(),
                hit_count=0,
                user_id=user_id
            )

            # è®¡ç®—èŠ‚çœæˆæœ¬
            cost_per_1k = self.COST_PER_1K_TOKENS.get(provider, 0.02)
            cost_saved = (tokens_used / 1000) * cost_per_1k
            self.stats.total_cost_saved_usd += cost_saved

            # è®¡ç®—èŠ‚çœå»¶è¿Ÿ
            latency_saved = self.LATENCY_ESTIMATE_MS.get(provider, 2000)
            self.stats.total_latency_saved_ms += latency_saved

            # å†™å…¥L1 (Redis)
            await self._write_to_redis(
                query,
                cache_entry,
                user_id,
                conversation_history,
                ttl
            )

            # å†™å…¥L2 (Qdrant) - ä»…å¯¹å¤æ‚åº¦>=3çš„é—®é¢˜
            if complexity >= 3:
                await self._write_to_qdrant(query, cache_entry, user_id)

            logger.debug(
                f"ğŸ’¾ Cache SET | "
                f"Query: {query[:30]}... | "
                f"Provider: {provider} | "
                f"Complexity: {complexity}"
            )

        except Exception as e:
            logger.error(f"Cache set error: {e}")

    async def _write_to_redis(
        self,
        query: str,
        cache_entry: CacheEntry,
        user_id: str,
        conversation_history: Optional[List[Dict]],
        ttl: int
    ):
        """å†™å…¥Redis (L1)"""
        try:
            context_hash = self._calculate_context_hash(conversation_history)
            cache_key = self._generate_cache_key(query, user_id, context_hash)

            await self.redis_manager.set(
                cache_key,
                json.dumps(cache_entry.to_dict()),
                ttl=ttl
            )
        except Exception as e:
            logger.error(f"Redis write error: {e}")

    async def _write_to_qdrant(
        self,
        query: str,
        cache_entry: CacheEntry,
        user_id: str
    ):
        """å†™å…¥Qdrant (L2)"""
        try:
            # ç¼–ç æŸ¥è¯¢å‘é‡
            loop = asyncio.get_event_loop()
            query_vector = await loop.run_in_executor(
                None,
                lambda: self.sentence_transformer.encode(
                    query,
                    convert_to_tensor=False
                ).tolist()
            )

            # ç”¨æˆ·ä¸“å±collection
            collection_name = f"cache_user_{user_id}"

            # ç¡®ä¿collectionå­˜åœ¨
            try:
                self.qdrant_client.get_collection(collection_name)
            except:
                self.qdrant_client.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(
                        size=384,
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"âœ… Created Qdrant collection '{collection_name}'")

            # ç”Ÿæˆç‚¹ID
            point_id = hashlib.md5(
                f"{user_id}:{query}".encode()
            ).hexdigest()

            # æ’å…¥å‘é‡
            self.qdrant_client.upsert(
                collection_name=collection_name,
                points=[
                    PointStruct(
                        id=point_id,
                        vector=query_vector,
                        payload=cache_entry.to_dict()
                    )
                ]
            )

        except Exception as e:
            logger.error(f"Qdrant write error: {e}")

    async def invalidate_user_cache(self, user_id: str):
        """
        æ¸…ç©ºç”¨æˆ·ç¼“å­˜

        Args:
            user_id: ç”¨æˆ·ID
        """
        try:
            # æ¸…ç©ºRedis
            pattern = f"response:{user_id}:*"
            deleted_count = await self.redis_manager.delete_pattern(pattern)

            # æ¸…ç©ºQdrant collection
            collection_name = f"cache_user_{user_id}"
            try:
                self.qdrant_client.delete_collection(collection_name)
            except:
                pass

            logger.info(
                f"ğŸ—‘ï¸  Cache invalidated for user {user_id} | "
                f"Deleted {deleted_count} Redis keys"
            )

        except Exception as e:
            logger.error(f"Cache invalidation error: {e}")

    def _update_cache_latency_stats(self, latency_ms: float, cached: bool):
        """æ›´æ–°å»¶è¿Ÿç»Ÿè®¡"""
        if cached:
            total_cached = (
                self.stats.l1_hits +
                self.stats.l2_hits +
                self.stats.l3_hits
            )
            if total_cached > 0:
                self.stats.avg_response_time_cached_ms = (
                    (self.stats.avg_response_time_cached_ms * (total_cached - 1) + latency_ms)
                    / total_cached
                )
        else:
            if self.stats.cache_misses > 0:
                self.stats.avg_response_time_uncached_ms = (
                    (self.stats.avg_response_time_uncached_ms * (self.stats.cache_misses - 1) + latency_ms)
                    / self.stats.cache_misses
                )

    def get_stats(self) -> dict:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total = self.stats.total_requests
        if total == 0:
            return {
                "message": "No requests yet",
                "total_requests": 0
            }

        total_hits = (
            self.stats.l1_hits +
            self.stats.l2_hits +
            self.stats.l3_hits
        )

        return {
            "total_requests": total,
            "total_hits": total_hits,
            "cache_hit_rate_percent": round(total_hits / total * 100, 2),
            "l1_hits": self.stats.l1_hits,
            "l1_hit_rate_percent": round(self.stats.l1_hits / total * 100, 2),
            "l2_hits": self.stats.l2_hits,
            "l2_hit_rate_percent": round(self.stats.l2_hits / total * 100, 2),
            "l3_hits": self.stats.l3_hits,
            "l3_hit_rate_percent": round(self.stats.l3_hits / total * 100, 2),
            "cache_misses": self.stats.cache_misses,
            "cache_miss_rate_percent": round(self.stats.cache_misses / total * 100, 2),
            "total_cost_saved_usd": round(self.stats.total_cost_saved_usd, 4),
            "total_latency_saved_ms": round(self.stats.total_latency_saved_ms, 2),
            "avg_cached_response_time_ms": round(self.stats.avg_response_time_cached_ms, 2),
            "avg_uncached_response_time_ms": round(self.stats.avg_response_time_uncached_ms, 2)
        }


# å…¨å±€å•ä¾‹
_cache_manager: Optional[ResponseCacheManager] = None


async def get_response_cache_manager() -> ResponseCacheManager:
    """
    è·å–å…¨å±€ResponseCacheManagerå•ä¾‹

    Returns:
        ResponseCacheManagerå®ä¾‹
    """
    global _cache_manager

    if _cache_manager is None:
        _cache_manager = ResponseCacheManager()
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œåˆå§‹åŒ–ï¼Œæ‡’åŠ è½½åˆ°é¦–æ¬¡ä½¿ç”¨æ—¶

    return _cache_manager
